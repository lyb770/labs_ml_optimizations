{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello world\n",
    "## Optimization algorithms: demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em><b>Note</b>: This is just a demo of optimization techniques - not related to any real-life problem.</em>\n",
    "\n",
    "Imagine the world of strings where the environment demands that the strings are very fit. An unfit string cannot survive. \n",
    "\n",
    "How do we define the fitness of a string? The environment demands that the string should be as similar as possible to the target string $s$=''Hello, world''. The closer the string is to ''Hello, world'', the better its chances of survival.\n",
    "\n",
    "<b> The problem:</b> given a set of random strings of length len($s$) over alphabet $\\sigma=\\{32 \\ldots 122\\}$, produce the string which best matches the environment, i.e. with a minimum fitness score. \n",
    "\n",
    "Let <em>Weighted Hamming Distance</em> between two strings $x$ and $y$ (both of length $n$) be defined as:<br>\n",
    "$WH(x,y) = \\sum_{i=0}^{n} abs(x[i] - y[i])$\n",
    "\n",
    "$WH(x,y)$ estimates how far is $x$ from $y$. The lower $WH(x,y)$, the closer string $x$ approaches the target string $y$.<br>\n",
    "In the space of $\\sigma^n$ of all possible strings, we are looking for a global minimum - the string $m$ such that $WH(m,s)$ is minimized. <br>\n",
    "\n",
    "If we compute $WH(x,s)$ for an arbitrary string $x$, we say that we evaluate the <em>fitness</em> of $x$ with respect to the target string $s$. This is our <em>fitness function</em>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag which can be turned on/off to print the steps of each optimization\n",
    "print_steps = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. String fitness function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Hello, world'\n",
    "n = len(s)\n",
    "\n",
    "# fitness function - weighted hamming distance\n",
    "string_fitness = lambda x: sum([abs(ord(x[i])- ord(s[i])) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Generating initial random strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "alphabet = range(32,123)\n",
    "\n",
    "def get_random_string(n, sigma):    \n",
    "    t = [chr(random.choice(sigma)) for i in range(n)]\n",
    "    return ''.join(t)\n",
    "\n",
    "# build initial population of many random strings of target length n\n",
    "def get_rand_population(population_size, n, sigma):\n",
    "    population=[]\n",
    "    for i in range(0,population_size):\n",
    "        individual_str = get_random_string(n, sigma)\n",
    "        population.append(individual_str)\n",
    "    return population\n",
    "\n",
    "# test\n",
    "print(get_random_string(len(s), alphabet))\n",
    "print(get_rand_population(3, len(s), alphabet ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random optimize\n",
    "\n",
    "The simplest possible idea is to try a vaste amount of random solutions and select the one with the best fitness score. Let's see how close we can get to the target $s$=''Hello, world'' with this approach.\n",
    "\n",
    "After all, isn't evolution just a lottery? ''Physics makes rules, evolution rolls the dice'' ( Charles Cockell. \"The Equations of Life\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_optimize(population, fitness_function):\n",
    "    best_score = None\n",
    "    best_solution = None\n",
    "    for i in range(len(population)):\n",
    "        # Calculate fitness \n",
    "        score = fitness_function(population[i])\n",
    "\n",
    "        # Compare it to the best one so far\n",
    "        if best_score == None or score < best_score:\n",
    "            best_score = score\n",
    "            best_solution = population[i]\n",
    "            if print_steps:\n",
    "                print(best_solution, \" -> score:\", best_score)\n",
    "            \n",
    "    return (best_score, best_solution, len(population))\n",
    "\n",
    "\n",
    "# Now the test\n",
    "string_population = get_rand_population(204800, len(s), alphabet)\n",
    "\n",
    "best_score, best_sol, iterations = random_optimize (string_population, string_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"*************Rand optimize**************\")\n",
    "print(\"trials:{}, best solution:'{}', best score:{}\".format(iterations,best_sol,best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly trying different solutions is very inefficient because the probability of getting anything close to perfect Hello, world is $(\\frac{1}{90})^{12}$. No matter how many random guesses you try â€“ the fitness of the resulting string is very low (the distance from the target remains high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hill climbing\n",
    "\n",
    "We did not advance far with the random optimization. This happens because each time we come up with some solution and evaluate its fitness - we discard it and try another one - not related to the previous.\n",
    "\n",
    "The idea behing the <em>hill climbing</em> optimization is to start at a random point in space (choose one random string), and try to move from this point into a direction of better fitness. We will generate a set of neighbors for a randomly selected string, and see if any of these neighbors improve the overall fitness score of the string. \n",
    "\n",
    "We continue moving into the direction of this better solution, until there is no more improvement. \n",
    "\n",
    "<img src=\"images/hillclimbing.jpg\" style=\"height:200px;\">\n",
    "\n",
    "\n",
    "The algorithm may produce an optimal solution, but it may also get stuck at the local minimum.\n",
    "<img src=\"images/localmin.jpg\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hillclimb_optimize(start_point, fitness_function):\n",
    "    sol = start_point\n",
    "    score = fitness_function(sol)\n",
    "    iterations = 0\n",
    "    \n",
    "    # Main loop\n",
    "    while 1:\n",
    "        iterations += 1\n",
    "        \n",
    "        current_best_score = score\n",
    "        # Create  neighboring solutions\n",
    "        neighbors=[]\n",
    "\n",
    "        for j in range(len(s)):\n",
    "            # Change character at position j one up or one down\n",
    "            if ord(sol[j])>32:\n",
    "                neighbors.append(sol[0:j]+chr(ord(sol[j])-1)+sol[j+1:])\n",
    "            if ord(sol[j])<122:\n",
    "                neighbors.append(sol[0:j]+chr(ord(sol[j])+1)+sol[j+1:])\n",
    "        \n",
    "        for n_str in neighbors:\n",
    "            n_score = fitness_function(n_str)\n",
    "            if n_score < score:\n",
    "                score = n_score\n",
    "                sol = n_str\n",
    "                if print_steps:\n",
    "                    print(sol, \" -> score:\", score)      \n",
    "        \n",
    "        # If there's no improvement, then we've reached the bottom of the hill\n",
    "        if score == current_best_score:\n",
    "            break        \n",
    "        \n",
    "        if score == 0:\n",
    "            break        \n",
    "        \n",
    "    return (score, sol, iterations)\n",
    "\n",
    "# Now the test\n",
    "\n",
    "rand_str = get_random_string(len(s), alphabet)\n",
    "best_score, best_sol, iterations = hillclimb_optimize(rand_str, string_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"*************Hill climbing****************\")\n",
    "print(\"steps:{}, best solution:'{}', best score:{}\".format(iterations,best_sol,best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulated annealing\n",
    "The idea of the <em>simulated annealing</em> is borrowed from physics. In metalurgical annealing we heat a metal (alloy) to a very high temperature, the crystal bonds break, and the atoms diffuse more freely. If we cool it slowly, the atoms tend to form more regular crystals producing an alloy with the low termodynamics energy.  \n",
    "\n",
    "<img style=\"height:250px;float:right;padding:4px;\" src=\"images/sim_annealing.jpg\" >\n",
    "\n",
    "In the <em>simulated annealing</em> algorithm we set the initial temperature very high, and then we generate a single random neighbor of the current solution. The fitness of this neihgbor can be better or worse that that of the current solution. When the temperature is high, the probability of selecting worse solution is higher. This allows to better explore the search space and get out of the local minimum. The temperature gradually decreases, and so at the end we do not accept worse solutions.\n",
    "\n",
    "The criterion of accepting ''bad'' solutions:\n",
    "\n",
    "$p=e^{\\frac{-\\Delta F}{T}}>R(0,1)$\n",
    "\n",
    "where $T$ is the current temperature, $R(0,1)$ is a random number between $0$ and $1$, and $\\Delta F$ is the difference between the fitness score of new solution and the old solution.\n",
    "\n",
    "Since the temperature $T$ (the willingness to accept a worse solution) starts very high,\n",
    "the exponent will be close to 0, and $p$ will almost be 1. As the temperature decreases, the difference between the new fitness score and the old one becomes more important - a bigger difference leads to a lower probability, so the\n",
    "algorithm will not accept solutions which do not improve fitness - converging to a local minimum after exploring a large global search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def annealing_optimize(start_sol, fitness_function, T=10000.0, cool=0.95, step=1):    \n",
    "    sol = start_sol\n",
    "    iterations = 0\n",
    "    \n",
    "    # Graduate cooling\n",
    "    while T > 0.01: \n",
    "        score = fitness_function(sol)\n",
    "        \n",
    "        # Choose one of spots randomly\n",
    "        i = random.randint(0, len(sol) - 1)\n",
    "\n",
    "        # Choose rand direction to change the character at position i\n",
    "        dir = random.randint(-step, step)\n",
    "\n",
    "        change = ord(sol[i]) + dir\n",
    "        # out of domain\n",
    "        if change > 122 or change < 32:\n",
    "            continue\n",
    "    \n",
    "        iterations += 1\n",
    "        \n",
    "        # Create a new solution with one of the characters changed\n",
    "        new_sol = sol[:i] + chr(change) + sol[i+1:]\n",
    "\n",
    "        # Calculate the new cost\n",
    "        new_score = fitness_function(new_sol)        \n",
    "\n",
    "        # Does it make the probability cutoff?\n",
    "        p = pow(math.e, -(new_score - score) / T)\n",
    "        \n",
    "        if new_score < score or p > random.random() :\n",
    "            sol = new_sol\n",
    "            score = new_score\n",
    "            if print_steps:\n",
    "                print(sol, \"-> score:\", score)       \n",
    "        \n",
    "        if score == 0:\n",
    "            break\n",
    "            \n",
    "        # Decrease the temperature\n",
    "        T = T * cool\n",
    "    \n",
    "    return (score, sol, iterations)\n",
    "\n",
    "# now test\n",
    "rand_str = get_random_string(len(s), alphabet)\n",
    "best_score, best_sol, iterations = annealing_optimize(rand_str, string_fitness,\n",
    "                                          T=204800.0, cool=0.999, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"*************Simulated annealing***************\")\n",
    "print(\"steps:{}, best solution:'{}', best score:{}\".format(iterations,best_sol,best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Genetic algorithm\n",
    "\n",
    "This optimization technique is inspired by the theory of evolution. The algorithm starts with a population of random individuals, and selects the ones with the best fitness score (elite). It continues to the next generation with this group. In order to enrich the genetic pool in the current generation, the algorithm adds random mutations and crossover to the elite group. After predefined number of generations, the algorithm returns the top-fit individual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Mutation and crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutation Operation\n",
    "def string_mutation(individual):\n",
    "    i = random.randint(0, len(individual) - 1)\n",
    "\n",
    "    # mutation changes character at random position to any valid character   \n",
    "    rchar = chr(random.randint(32, 122))\n",
    "\n",
    "    individual = individual[0:i] + rchar + individual[(i + 1):]\n",
    "    return individual\n",
    "\n",
    "\n",
    "# Mate operation (crossover)\n",
    "def string_crossover (s1, s2):\n",
    "    # find the point of crossover\n",
    "    i = random.randint (0, len(s1)-1)\n",
    "    return s1[:i]+s2[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Algorithm\n",
    "\n",
    "Initial population - a list of random strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_optimize(population, fitness_function,\n",
    "                     mutation_function, mate_function,\n",
    "                     mutation_probability, elite_ratio,\n",
    "                     maxiterations):\n",
    "\n",
    "    # How many winners to consider from each generation?\n",
    "    original_population_size = len(population)\n",
    "    top_elite = int(elite_ratio * original_population_size)\n",
    "\n",
    "    # Main loop\n",
    "    iterations = 0\n",
    "    for i in range(maxiterations):\n",
    "        iterations += 1\n",
    "        individual_scores = [(fitness_function(v), v) for v in population]\n",
    "        individual_scores.sort()\n",
    "        ranked_individuals = [v for (s, v) in individual_scores]\n",
    "\n",
    "        # Start with the pure winners\n",
    "        population = ranked_individuals[0:top_elite]\n",
    "\n",
    "        # Add mutated and bred forms of the winners\n",
    "        while len(population) < original_population_size:\n",
    "            if random.random() < mutation_probability:\n",
    "                # Mutation\n",
    "                c = random.randint(0, top_elite)\n",
    "                population.append(mutation_function(ranked_individuals[c]))\n",
    "            else:\n",
    "                # Crossover\n",
    "                c1 = random.randint(0, top_elite)\n",
    "                c2 = random.randint(0, top_elite)\n",
    "                population.append(mate_function(ranked_individuals[c1], ranked_individuals[c2]))\n",
    "\n",
    "        # Print current best score\n",
    "        if print_steps:\n",
    "            print(individual_scores[0][1],\" -> score:\", individual_scores[0][0])\n",
    "\n",
    "        if individual_scores[0][0] == 0:\n",
    "            return (individual_scores[0][0],individual_scores[0][1], iterations)\n",
    "\n",
    "    # returns the best solution\n",
    "    return (individual_scores[0][0], individual_scores[0][1], iterations)\n",
    "\n",
    "\n",
    "\n",
    "string_population = get_rand_population(2048, len(s), alphabet)\n",
    "best_score, best_sol, iterations  = genetic_optimize(string_population, string_fitness,\n",
    "                                string_mutation, string_crossover,\n",
    "                                mutation_probability=0.25, elite_ratio=0.1,\n",
    "                                maxiterations=100)\n",
    "print()\n",
    "print(\"*****************GENETIC ALGORITHM ***************\")\n",
    "print(\"generations:{}, best solution:'{}', best score:{}\".format(iterations,best_sol,best_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of the ''Hello, world'' demo.\n",
    "Copyright &copy; 2022 Marina Barsky"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
